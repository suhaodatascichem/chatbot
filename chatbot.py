# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuJZ856fzTikTwQLTmORJgxvWDubQNSd
"""

import streamlit as st
from docx import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import  Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from transformers import pipeline
import tempfile
import os

# Load QA model
@st.cache_resource
def load_qa_pipeline():
    return pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

qa = load_qa_pipeline()

# App UI
st.title("ðŸ“„ Evonik Doc Chatbot")
uploaded_file = st.file_uploader("Upload your doc (.docx)", type=["docx"])

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    def load_docx(path):
        doc = Document(path)
        return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

    doc_text = load_docx(tmp_path)

    # Split + embed
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    chunks = splitter.split_text(doc_text)
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = Chroma.from_texts(chunks, embedding_model)

    # Ask
    user_query = st.text_input("Ask a question about the doc:")
    if user_query:
        docs = db.similarity_search(user_query, k=3)
        context = "\n\n".join([doc.page_content for doc in docs])

        with st.spinner("Thinking..."):
            result = qa(question=user_query, context=context)
            st.markdown("### ðŸ§  Answer:")
            st.write(result["answer"])

        with st.expander("Show context used"):
            st.write(context)

    os.remove(tmp_path)