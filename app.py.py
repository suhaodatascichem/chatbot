# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuJZ856fzTikTwQLTmORJgxvWDubQNSd
"""

import streamlit as st
from docx import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from llama_cpp import Llama
import tempfile
import os

# Load TinyLLaMA model (once only)
@st.cache_resource
def load_llm():
    return Llama(
        model_path="tinyllama.gguf",  # Make sure this file exists
        n_ctx=2048,
        n_threads=4
    )

llm = load_llm()

# File uploader
st.title("ðŸ“„ Evonik SOP Chatbot (TinyLLaMA)")
uploaded_file = st.file_uploader("Upload your SOP (.docx)", type=["docx"])

if uploaded_file:
    # Save temp file
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    # Load docx
    def load_docx(path):
        doc = Document(path)
        return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])

    doc_text = load_docx(tmp_path)

    # Text splitting
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    chunks = splitter.split_text(doc_text)

    # Embedding & vector DB
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    db = FAISS.from_texts(chunks, embedding_model)

    # Query UI
    user_query = st.text_input("Ask a question about the SOP:")
    if user_query:
        docs = db.similarity_search(user_query, k=3)
        context = "\n\n".join([doc.page_content for doc in docs])

        prompt = f"""Use the following context to answer the question.

Context:
{context}

Question: {user_query}
Answer:"""

        with st.spinner("Thinking..."):
            response = llm(prompt, max_tokens=256, temperature=0.7, top_p=0.9, stop=["\n\n", "</s>"])
            answer = response["choices"][0]["text"].strip()

        st.markdown("### ðŸ§  Answer:")
        st.write(answer)

        # Optional: show the retrieved context
        with st.expander("Show context used"):
            st.write(context)

    # Cleanup
    os.remove(tmp_path)