# -*- coding: utf-8 -*-
"""chatbot-DocArrayInMemory.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuJZ856fzTikTwQLTmORJgxvWDubQNSd
"""

import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.document_loaders import Docx2txtLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import pipeline
import tempfile
import os

# Initialize constants
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL_NAME = "google/flan-t5-base"  # or "google/flan-t5-small" for lighter version

st.set_page_config(page_title="Document QA Chatbot", layout="wide")
st.title("üìÑ Document QA Chatbot (In-Memory)")

# Load LLM pipeline
@st.cache_resource
def load_llm():
    pipe = pipeline(
        "text2text-generation",
        model=LLM_MODEL_NAME,
        tokenizer=LLM_MODEL_NAME,
        max_length=512,
        temperature=0.0
    )
    return HuggingFacePipeline(pipeline=pipe)

# Embed and store paragraphs
def embed_and_store_paragraphs(paragraphs: list[str]):
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    return DocArrayInMemorySearch.from_texts(paragraphs, embedding=embeddings)

# Load and split Word document
def load_and_split_doc(doc_path):
    loader = Docx2txtLoader(doc_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    return splitter.split_documents(docs)

# UI: Upload DOCX file
uploaded_file = st.file_uploader("Upload a Word Document (.docx)", type=["docx"])

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".docx") as tmp:
        tmp.write(uploaded_file.read())
        tmp_path = tmp.name

    st.success("‚úÖ Document uploaded successfully!")

    # Load & process the document
    with st.spinner("üîç Processing document..."):
        docs = load_and_split_doc(tmp_path)
        paragraphs = [doc.page_content for doc in docs]
        vector_store = embed_and_store_paragraphs(paragraphs)
        retriever = vector_store.as_retriever(search_kwargs={"k": 3})
        qa_chain = RetrievalQA.from_chain_type(
            llm=load_llm(),
            retriever=retriever,
            chain_type="stuff"
        )

    # UI: Ask a question
    question = st.text_input("Ask a question about the document:")
    if question:
        with st.spinner("ü§ñ Generating answer..."):
            answer = qa_chain.run(question)
        st.markdown("### üí¨ Answer:")
        st.write(answer)

    # Clean up temp file
    os.unlink(tmp_path)
else:
    st.info("‚¨ÜÔ∏è Please upload a .docx file to get started.")