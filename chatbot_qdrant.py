# -*- coding: utf-8 -*-
"""chatbot-qdrant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FuJZ856fzTikTwQLTmORJgxvWDubQNSd
"""

import streamlit as st
from docx import Document
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.http.models import VectorParams, Distance
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from transformers import pipeline

# -------------------- CONFIG --------------------
QDRANT_URL = "https://cloud.qdrant.io/accounts/ec1ea4b3-1b51-4437-9f88-463c24ad549e/clusters/08f4bc61-015a-4155-8079-03e0c578fc93/overview"  # ðŸ” Replace this
QDRANT_API_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.HsR8zBMJCg-3eQeYgu76_WgJzzfSBQnVaCQEmEQOL1Q"                   # ðŸ” Replace this
COLLECTION_NAME = "company_doc"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"

# -------------------- UI --------------------
st.title("ðŸ“„ Company  Chatbot")
st.markdown("Upload a `.docx` file and ask questions about it.")

uploaded_file = st.file_uploader("Upload doc (Word Document)", type=["docx"])
query = st.text_input("Ask a question about the doc:")

# -------------------- Functions --------------------
def extract_text_from_docx(file) -> list[str]:
    doc = Document(file)
    return [para.text.strip() for para in doc.paragraphs if para.text.strip()]

def embed_and_store_paragraphs(paragraphs: list[str]):
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    vector_dim = len(embeddings.embed_documents(["test"])[0])

    client = QdrantClient(
        url=QDRANT_URL,
        api_key=QDRANT_API_KEY,
    )

    client.recreate_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=vector_dim, distance=Distance.COSINE),
    )

    vectors = embeddings.embed_documents(paragraphs)
    payload = [{"text": p} for p in paragraphs]
    ids = list(range(len(paragraphs)))

    client.upload_collection(
        collection_name=COLLECTION_NAME,
        vectors=vectors,
        payload=payload,
        ids=ids
    )

    return Qdrant(client=client, collection_name=COLLECTION_NAME, embeddings=embeddings)

def build_qa_chain(vector_store):
    llm_pipeline = pipeline("text-generation", model="tiiuae/falcon-7b-instruct", device=0)
    llm = HuggingFacePipeline(pipeline=llm_pipeline)
    return RetrievalQA.from_chain_type(llm=llm, retriever=vector_store.as_retriever())

# -------------------- Logic --------------------
if uploaded_file:
    paragraphs = extract_text_from_docx(uploaded_file)
    st.success(f"Extracted {len(paragraphs)} paragraphs.")
    st.session_state.vector_store = embed_and_store_paragraphs(paragraphs)
    st.success("doc embedded and stored in Qdrant!")

if query and "vector_store" in st.session_state:
    qa_chain = build_qa_chain(st.session_state.vector_store)
    with st.spinner("Thinking..."):
        answer = qa_chain.run(query)
    st.write("### ðŸ¤– Answer:")
    st.write(answer)